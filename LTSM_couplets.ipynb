{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from couplets_utils import *\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_A = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = load_sample_datasets(vocabs_size=2000, max_len=30, batch_size=16, sample_size=1000, n_a=N_A)\n",
    "sample_gen = sample['sample_gen']\n",
    "index2word = sample['index2word']\n",
    "word2index = sample['word2index']\n",
    "vocabs_size = sample['vocabs_size']\n",
    "max_len = sample['max_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_gen length : 63\n"
     ]
    }
   ],
   "source": [
    "print('sample_gen length :', len(sample_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_train_model(n_x, n_a, Tx, keep_prob=0.5):    \n",
    "    input = Input(shape=(Tx, n_x), name='x0')\n",
    "    a_0_0 = Input(shape=(n_a,), name='a_0_0')\n",
    "    c_0_0 = Input(shape=(n_a,), name='c_0_0')\n",
    "    a_1_0 = Input(shape=(n_a,), name='a_1_0')\n",
    "    c_1_0 = Input(shape=(n_a,), name='c_1_0')\n",
    "        \n",
    "    a_0 = a_0_0\n",
    "    c_0 = c_0_0\n",
    "    a_1 = a_1_0\n",
    "    c_1 = c_1_0\n",
    "    \n",
    "    lstm_cell_0 = LSTM(units=n_a, return_state=True, name='lstm_0')\n",
    "    lstm_cell_1 = LSTM(units=n_a, return_state=True, name='lstm_1')\n",
    "    dense_layer_2 = Dense(units=n_x, activation='softmax', name='softmax_2')\n",
    "        \n",
    "    outputs = []\n",
    "        \n",
    "    for i in range(Tx):\n",
    "        x = Lambda(lambda j: j[:, i, :])(input)\n",
    "        x = Reshape(target_shape=(1, -1))(x)\n",
    "        a_0, x, c_0 = lstm_cell_0(x, initial_state=[a_0, c_0])\n",
    "        x = Dropout(rate=1-keep_prob)(x)\n",
    "        x = Reshape(target_shape=(1, -1))(x)\n",
    "        a_1, x, c_1 = lstm_cell_1(x, initial_state=[a_1, c_1])\n",
    "        x = Dropout(rate=1-keep_prob)(x)\n",
    "        x = dense_layer_2(x)\n",
    "        outputs.append(x)\n",
    "                            \n",
    "    model = Model(inputs=[input, a_0_0, c_0_0, a_1_0, c_1_0], outputs=outputs)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_model = create_train_model(vocabs_size, N_A, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "63/63 [==============================] - 40s 633ms/step - loss: 156.5443 - softmax_2_loss_1: 7.5616 - softmax_2_loss_2: 7.5211 - softmax_2_loss_3: 7.4108 - softmax_2_loss_4: 7.2242 - softmax_2_loss_5: 6.8566 - softmax_2_loss_6: 7.0495 - softmax_2_loss_7: 7.7294 - softmax_2_loss_8: 5.9416 - softmax_2_loss_9: 7.6246 - softmax_2_loss_10: 7.5361 - softmax_2_loss_11: 7.4984 - softmax_2_loss_12: 7.0458 - softmax_2_loss_13: 6.4441 - softmax_2_loss_14: 6.7139 - softmax_2_loss_15: 6.5470 - softmax_2_loss_16: 4.9805 - softmax_2_loss_17: 3.9223 - softmax_2_loss_18: 3.4926 - softmax_2_loss_19: 3.8836 - softmax_2_loss_20: 3.6381 - softmax_2_loss_21: 3.6753 - softmax_2_loss_22: 3.5416 - softmax_2_loss_23: 3.5942 - softmax_2_loss_24: 3.4512 - softmax_2_loss_25: 3.3751 - softmax_2_loss_26: 2.8484 - softmax_2_loss_27: 2.5258 - softmax_2_loss_28: 2.3722 - softmax_2_loss_29: 2.2865 - softmax_2_loss_30: 2.2523 - softmax_2_acc_1: 0.0000e+00 - softmax_2_acc_2: 0.0000e+00 - softmax_2_acc_3: 9.9219e-04 - softmax_2_acc_4: 9.9219e-04 - softmax_2_acc_5: 9.9219e-04 - softmax_2_acc_6: 9.9219e-04 - softmax_2_acc_7: 0.0238 - softmax_2_acc_8: 0.0258 - softmax_2_acc_9: 0.0327 - softmax_2_acc_10: 0.0327 - softmax_2_acc_11: 0.0556 - softmax_2_acc_12: 0.0546 - softmax_2_acc_13: 0.1696 - softmax_2_acc_14: 0.1716 - softmax_2_acc_15: 0.1835 - softmax_2_acc_16: 0.1835 - softmax_2_acc_17: 0.6954 - softmax_2_acc_18: 0.6964 - softmax_2_acc_19: 0.7024 - softmax_2_acc_20: 0.6994 - softmax_2_acc_21: 0.7411 - softmax_2_acc_22: 0.7401 - softmax_2_acc_23: 0.7599 - softmax_2_acc_24: 0.7639 - softmax_2_acc_25: 0.7857 - softmax_2_acc_26: 0.7847 - softmax_2_acc_27: 0.9296 - softmax_2_acc_28: 0.9266 - softmax_2_acc_29: 0.9623 - softmax_2_acc_30: 0.9603\n"
     ]
    }
   ],
   "source": [
    "result = train_model.fit_generator(sample_gen, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'epoch',\n",
       " 'history',\n",
       " 'model',\n",
       " 'on_batch_begin',\n",
       " 'on_batch_end',\n",
       " 'on_epoch_begin',\n",
       " 'on_epoch_end',\n",
       " 'on_train_begin',\n",
       " 'on_train_end',\n",
       " 'params',\n",
       " 'set_model',\n",
       " 'set_params',\n",
       " 'validation_data']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_model.save_weights('./weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "infer_model = create_infer_model(vocabs_size, N_A, max_len)\n",
    "infer_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "infer_model.load_weights('./weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_couplets(begin_text, infer_model):\n",
    "    x = convert_text_to_onehot(begin_text, vocabs_size, max_len, word2index)\n",
    "    a_0 = np.zeros((1, N_A))\n",
    "    c_0 = np.zeros((1, N_A))\n",
    "    a_1 = np.zeros((1, N_A))\n",
    "    c_1 = np.zeros((1, N_A))\n",
    "    \n",
    "    result = infer_model.predict([x, a_0, c_0, a_1, c_1])\n",
    "    result_text = convert_predict_to_text(np.array(result), index2word)\n",
    "    \n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "天天向上                         \n"
     ]
    }
   ],
   "source": [
    "text = '天天向上'\n",
    "result_text = write_couplets(text, infer_model)\n",
    "print(result_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
